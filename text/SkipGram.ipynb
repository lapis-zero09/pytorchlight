{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T14:37:38.556372Z",
     "start_time": "2018-03-11T14:37:38.156999Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T14:54:45.135471Z",
     "start_time": "2018-03-11T14:54:44.999933Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, word_dim, use_gpu=False):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.word_dim = word_dim\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, word_dim, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, word_dim, sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.word_dim\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.zero_()\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        score = torch.mul(emb_u, emb_v).squeeze()\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = F.logsigmoid(score)\n",
    "        neg_emb_v = self.v_embeddings(neg_v)\n",
    "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
    "\n",
    "    def save_embedding(self, word2id, file_name, use_gpu=False):\n",
    "        if use_gpu:\n",
    "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        else:\n",
    "            embedding = self.u_embeddings.weight.data.numpy()\n",
    "            \n",
    "        fout = open(file_name, 'w')\n",
    "        fout.write('%d %d\\n' % (len(word2id), self.word_dim))\n",
    "        for word, word_id in word2id.items():\n",
    "            e = embedding[word_id]\n",
    "            e = ' '.join(map(lambda x: str(x), e))\n",
    "            fout.write('%s %s\\n' % (word, e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:04:12.880727Z",
     "start_time": "2018-03-11T15:04:12.499791Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessText():\n",
    "    def __init__(self, file_path, min_count):\n",
    "        from collections import deque\n",
    "        self.file_path = file_path\n",
    "        self.sentence_length = 0\n",
    "        self.build_vocab(min_count)\n",
    "        self.word_pair_catch = deque()\n",
    "        self.init_neg_sample_table()\n",
    "        print('Vocab size: %d' % self.vocab_size)\n",
    "        print('Sentence Length: %d' % self.sentence_length)\n",
    "\n",
    "    def get_clean_word(self, file, init=0):\n",
    "        \"\"\"\n",
    "        sentence_count\n",
    "        sentence_length\n",
    "        \"\"\"\n",
    "        import re\n",
    "        lines = file.readlines()\n",
    "        lines = [line for line in lines if len(line) > 1]\n",
    "        if init:\n",
    "            self.sentence_count = len(lines)\n",
    "        r = re.compile(\"[!-/:-@[-`{-~]\")\n",
    "        for line in map(lambda x: re.sub(r, '', x.lower().strip()).split(),\n",
    "                        lines):\n",
    "            if init:\n",
    "                self.sentence_length += len(line)\n",
    "            for word in line:\n",
    "                yield word\n",
    "\n",
    "    def build_vocab(self, min_count):\n",
    "        \"\"\"\n",
    "        word2id\n",
    "        word_list\n",
    "        word_count\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        vocab = Counter()\n",
    "        for word in self.get_clean_word(open(self.file_path), init=1):\n",
    "            vocab[word] += 1\n",
    "\n",
    "        self.freq = {k: v for k, v in vocab.items() if v >= min_count}\n",
    "        self.word_count = sum(self.freq.values())\n",
    "        word_list = sorted(self.freq, key=self.freq.get, reverse=True)\n",
    "        self.word2id = {w: i for i, w in enumerate(word_list)}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "\n",
    "    def init_neg_sample_table(self):\n",
    "        self.neg_sample_table = []\n",
    "        neg_sample_table_size = 1e8\n",
    "        pow_frequency = np.array(list(self.freq.values()))**0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = np.round(ratio * neg_sample_table_size)\n",
    "        for idx, c in enumerate(count):\n",
    "            self.neg_sample_table += [idx] * int(c)\n",
    "        self.neg_sample_table = np.array(self.neg_sample_table)\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "        while len(self.word_pair_catch) < batch_size:\n",
    "            word_ids = []\n",
    "            for word in self.get_clean_word(open(self.file_path)):\n",
    "                try:\n",
    "                    word_ids.append(self.word2id[word])\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            for i, u in enumerate(word_ids):\n",
    "                for j, v in enumerate(\n",
    "                        word_ids[max(i - window_size, 0):i + window_size]):\n",
    "                    assert u < self.vocab_size\n",
    "                    assert v < self.vocab_size\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    self.word_pair_catch.append((u, v))\n",
    "        batch_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            batch_pairs.append(self.word_pair_catch.popleft())\n",
    "        return batch_pairs\n",
    "\n",
    "    # @profile\n",
    "    def get_neg_v(self, pos_word_pair, count):\n",
    "        neg_v = np.random.choice(\n",
    "            self.neg_sample_table, size=(len(pos_word_pair), count)).tolist()\n",
    "        return neg_v\n",
    "\n",
    "    def evaluate_pair_count(self, window_size):\n",
    "        return self.sentence_length * (2 * window_size - 1) - (\n",
    "            self.sentence_count - 1) * (1 + window_size) * window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:08:07.211084Z",
     "start_time": "2018-03-11T15:07:57.544987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 410\n",
      "Sentence Length: 29384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PreProcessText at 0x7fe35b5e34e0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PreProcessText('./data/alice.txt', 10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:09:39.978977Z",
     "start_time": "2018-03-11T15:09:39.937558Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "batch_size = 50\n",
    "window_size = 5\n",
    "iteration = 5\n",
    "initial_lr = 0.0025\n",
    "emb_size = len(data.word2id)\n",
    "\n",
    "model = SkipGram(emb_size, emb_dim)\n",
    "optimizer = optim.SparseAdam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "pair_count = data.evaluate_pair_count(window_size)\n",
    "batch_count = iteration * pair_count / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:09:40.498894Z",
     "start_time": "2018-03-11T15:09:40.494774Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:10:49.655093Z",
     "start_time": "2018-03-11T15:09:40.851572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f13afc7d8e40d8be06175cc979a730"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lapis_zero09/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/lapis_zero09/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/lapis_zero09/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_bar = tnrange(int(batch_count))\n",
    "for i in process_bar:\n",
    "    pos_pairs = data.get_batch_pairs(batch_size, window_size)\n",
    "    neg_v = data.get_neg_v(pos_pairs, 5)\n",
    "    pos_u = [pair[0] for pair in pos_pairs]\n",
    "    pos_v = [pair[1] for pair in pos_pairs]\n",
    "\n",
    "    pos_u = Variable(torch.LongTensor(pos_u))\n",
    "    pos_v = Variable(torch.LongTensor(pos_v))\n",
    "    neg_v = Variable(torch.LongTensor(neg_v))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forward(pos_u, pos_v, neg_v)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
    "                                (loss.data[0],\n",
    "                                 optimizer.param_groups[0]['lr']))\n",
    "    if i * batch_size % 256 == 0:\n",
    "        lr = initial_lr * (1.0 - 1.0 * i / batch_count)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:12:02.031008Z",
     "start_time": "2018-03-11T15:12:01.882015Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_embedding(data.word2id, './data/skipgram.w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:06:11.245941Z",
     "start_time": "2018-03-11T15:06:11.241227Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:12:08.402563Z",
     "start_time": "2018-03-11T15:12:08.270793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mock', 0.9239335656166077),\n",
       " ('turtle', 0.9169539213180542),\n",
       " ('hatter', 0.9001433849334717),\n",
       " ('gryphon', 0.8870241045951843),\n",
       " ('hare', 0.8557647466659546),\n",
       " ('march', 0.8521391153335571),\n",
       " ('queen', 0.8199282288551331),\n",
       " ('who', 0.802047610282898),\n",
       " ('tea', 0.765494704246521),\n",
       " ('jury', 0.7476778626441956)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('./data/skipgram.w', binary=False)\n",
    "word_vectors.most_similar(positive=['king'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
