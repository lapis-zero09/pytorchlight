{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T15:34:58.357950Z",
     "start_time": "2018-03-12T15:34:57.940941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T15:34:59.096959Z",
     "start_time": "2018-03-12T15:34:58.967578Z"
    }
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, word_dim, use_gpu=False):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.word_dim = word_dim\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, word_dim, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, word_dim, sparse=True)\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.word_dim\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.zero_()\n",
    "\n",
    "    def forward(self, pos_u, pos_v, neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        emb_v = self.v_embeddings(pos_v)\n",
    "        score = torch.mul(emb_u, emb_v).squeeze()\n",
    "        score = torch.sum(score, dim=1)\n",
    "        score = F.logsigmoid(score)\n",
    "        neg_emb_v = self.v_embeddings(neg_v)\n",
    "        neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        neg_score = F.logsigmoid(-1 * neg_score)\n",
    "        return -1 * (torch.sum(score)+torch.sum(neg_score))\n",
    "\n",
    "    def save_embedding(self, word2id, file_name, use_gpu=False):\n",
    "        if use_gpu:\n",
    "            embedding = self.u_embeddings.weight.cpu().data.numpy()\n",
    "        else:\n",
    "            embedding = self.u_embeddings.weight.data.numpy()\n",
    "            \n",
    "        fout = open(file_name, 'w')\n",
    "        fout.write('%d %d\\n' % (len(word2id), self.word_dim))\n",
    "        for word, word_id in word2id.items():\n",
    "            e = embedding[word_id]\n",
    "            e = ' '.join(map(lambda x: str(x), e))\n",
    "            fout.write('%s %s\\n' % (word, e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:21:59.508551Z",
     "start_time": "2018-03-12T16:21:59.102824Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessText():\n",
    "    def __init__(self, file_path, min_count):\n",
    "        from collections import deque\n",
    "        self.file_path = file_path\n",
    "        self.sentence_length = 0\n",
    "        self.build_vocab(min_count)\n",
    "        self.word_pair_catch = deque()\n",
    "        self.init_neg_sample_table()\n",
    "        print('Vocab size: %d' % self.vocab_size)\n",
    "        print('Sentence Length: %d' % self.sentence_length)\n",
    "\n",
    "    def get_clean_word(self, file, init=0):\n",
    "        \"\"\"\n",
    "        sentence_count\n",
    "        sentence_length\n",
    "        \"\"\"\n",
    "        import re\n",
    "        lines = file.readlines()\n",
    "        lines = [line for line in lines if len(line) > 1]\n",
    "        if init:\n",
    "            self.sentence_count = len(lines)\n",
    "        r = re.compile(\"[!-/:-@[-`{-~]\")\n",
    "        for line in map(lambda x: re.sub(r, '', x.lower().strip()).split(),\n",
    "                        lines):\n",
    "            if init:\n",
    "                self.sentence_length += len(line)\n",
    "            for word in line:\n",
    "                yield word\n",
    "\n",
    "    def build_vocab(self, min_count):\n",
    "        \"\"\"\n",
    "        word2id\n",
    "        word_list\n",
    "        word_count\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        vocab = Counter()\n",
    "        for word in self.get_clean_word(open(self.file_path), init=1):\n",
    "            vocab[word] += 1\n",
    "\n",
    "        self.freq = {k: v for k, v in vocab.items() if v >= min_count}\n",
    "        self.word_count = sum(self.freq.values())\n",
    "        word_list = sorted(self.freq, key=self.freq.get, reverse=True)\n",
    "        self.word2id = {w: i for i, w in enumerate(word_list)}\n",
    "        self.vocab_size = len(self.word2id)\n",
    "\n",
    "    def init_neg_sample_table(self):\n",
    "        self.neg_sample_table = []\n",
    "        neg_sample_table_size = 1e8\n",
    "        pow_frequency = np.array(list(self.freq.values()))**0.75\n",
    "        words_pow = sum(pow_frequency)\n",
    "        ratio = pow_frequency / words_pow\n",
    "        count = np.round(ratio * neg_sample_table_size)\n",
    "        for idx, c in enumerate(count):\n",
    "            self.neg_sample_table += [idx] * int(c)\n",
    "        self.neg_sample_table = np.array(self.neg_sample_table)\n",
    "\n",
    "    def get_batch_pairs(self, batch_size, window_size):\n",
    "        while len(self.word_pair_catch) < batch_size:\n",
    "            word_ids = []\n",
    "            for word in self.get_clean_word(open(self.file_path)):\n",
    "                try:\n",
    "                    word_ids.append(self.word2id[word])\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            for i, u in enumerate(word_ids):\n",
    "                for j, v in enumerate(\n",
    "                        word_ids[max(i - window_size, 0):i + window_size]):\n",
    "                    assert u < self.vocab_size\n",
    "                    assert v < self.vocab_size\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    self.word_pair_catch.append((u, v))\n",
    "        batch_pairs = []\n",
    "        for _ in range(batch_size):\n",
    "            batch_pairs.append(self.word_pair_catch.popleft())\n",
    "        return batch_pairs\n",
    "\n",
    "    # @profile\n",
    "    def get_neg_v(self, batch_size, negative_sample_size):\n",
    "        neg_v = np.random.choice(\n",
    "            self.neg_sample_table, size=(batch_size,\n",
    "                                         negative_sample_size)).tolist()\n",
    "        return neg_v\n",
    "\n",
    "    def evaluate_pair_count(self, window_size):\n",
    "        return self.sentence_length * (2 * window_size - 1) - (\n",
    "            self.sentence_count - 1) * (1 + window_size) * window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:22:10.002917Z",
     "start_time": "2018-03-12T16:22:00.461209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 410\n",
      "Sentence Length: 29384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.PreProcessText at 0x7ffb50acab70>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = PreProcessText('./data/alice.txt', 10)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:55:50.228170Z",
     "start_time": "2018-03-12T16:55:50.184609Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "batch_size = 50\n",
    "window_size = 5\n",
    "negative_sample_size = 5\n",
    "iteration = 5\n",
    "initial_lr = 0.0025\n",
    "emb_size = len(data.word2id)\n",
    "\n",
    "model = SkipGram(emb_size, emb_dim)\n",
    "optimizer = optim.SparseAdam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "pair_count = data.evaluate_pair_count(window_size)\n",
    "batch_count = iteration * pair_count / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:55:50.616759Z",
     "start_time": "2018-03-12T16:55:50.612453Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:56:41.998703Z",
     "start_time": "2018-03-12T16:55:52.370860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b67e1be6ed480d9e64a1e479087a95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_bar = tnrange(int(batch_count))\n",
    "for i in process_bar:\n",
    "    pos_pairs = data.get_batch_pairs(batch_size, window_size)\n",
    "    neg_v = data.get_neg_v(batch_size, negative_sample_size)\n",
    "    pos_u = [pair[0] for pair in pos_pairs]\n",
    "    pos_v = [pair[1] for pair in pos_pairs]\n",
    "\n",
    "    pos_u = Variable(torch.LongTensor(pos_u))\n",
    "    pos_v = Variable(torch.LongTensor(pos_v))\n",
    "    neg_v = Variable(torch.LongTensor(neg_v))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = model.forward(pos_u, pos_v, neg_v)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "#     process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
    "#                                 (loss.data[0],\n",
    "#                                  optimizer.param_groups[0]['lr']))\n",
    "    if i * batch_size % 10000 == 0:\n",
    "        process_bar.set_description(\"Loss: %0.8f, lr: %0.6f\" %\n",
    "                            (loss.data[0],\n",
    "                             optimizer.param_groups[0]['lr']))\n",
    "        lr = initial_lr * (1.0 - 1.0 * i / batch_count)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:58:08.423097Z",
     "start_time": "2018-03-12T16:58:08.289211Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_embedding(data.word2id, './data/skipgram.w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:58:08.946863Z",
     "start_time": "2018-03-12T16:58:08.942450Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T16:58:09.464820Z",
     "start_time": "2018-03-12T16:58:09.306896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dormouse', 0.8835446834564209),\n",
       " ('his', 0.8777092099189758),\n",
       " ('king', 0.8710049986839294),\n",
       " ('who', 0.861481249332428),\n",
       " ('turtle', 0.8438349366188049),\n",
       " ('hatter', 0.8383009433746338),\n",
       " ('gryphon', 0.8233548402786255),\n",
       " ('gloves', 0.8195096254348755),\n",
       " ('mock', 0.8153071999549866),\n",
       " ('white', 0.8116865158081055)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('./data/skipgram.w', binary=False)\n",
    "word_vectors.most_similar(positive=['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
